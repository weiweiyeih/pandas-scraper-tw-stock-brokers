{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tutorial links: \n",
    "\n",
    "[Web Scraping with Pandas](https://www.youtube.com/watch?v=oF-EMiPZQGA)\n",
    "\n",
    "[【 台灣股市資訊網】Post爬蟲大公開](https://medium.com/pythonstock/%E5%8F%B0%E7%81%A3%E8%82%A1%E5%B8%82%E8%B3%87%E8%A8%8A%E7%B6%B2-post%E7%88%AC%E8%9F%B2%E5%A4%A7%E5%85%AC%E9%96%8B-%E9%99%84-python%E7%A8%8B%E5%BC%8F%E7%A2%BC-e296238f9ef4)\n",
    "\n",
    "[Sending \"User-agent\" using Requests library in Python](https://stackoverflow.com/questions/10606133/sending-user-agent-using-requests-library-in-python)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goal\n",
    "\n",
    "To scrape 2 webpages and save the combine the data into excel\n",
    "\n",
    "- [券商進出排行](https://5850web.moneydj.com/z/zg/zgb/zgb0.djhtm?a=1470&b=1470&c=B&d=1)\n",
    "\n",
    "- [Goodinfo!](https://goodinfo.tw/tw2/StockList.asp?MARKET_CAT=%E7%86%B1%E9%96%80%E6%8E%92%E8%A1%8C&INDUSTRY_CAT=%E6%88%90%E4%BA%A4%E5%83%B9+%28%E9%AB%98%E2%86%92%E4%BD%8E%29%40%40%E6%88%90%E4%BA%A4%E5%83%B9%40%40%E7%94%B1%E9%AB%98%E2%86%92%E4%BD%8E)\n",
    "\n",
    "![Optional Text](IMG_4189.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get 2 tables from [券商進出排行榜](https://5850web.moneydj.com/z/zg/zgb/zgb0.djhtm?a=1470&b=1470&c=B&d=1)\n",
    "\n",
    "Use Pandas scraper to get tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import re\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "url = \"https://5850web.moneydj.com/z/zg/zgb/zgb0.djhtm?a=1470&b=1470&c=B&d=1\"\n",
    "response = requests.get(url)\n",
    "response.encoding = 'big5'  # Specify the correct encoding here\n",
    "\n",
    "scraper = pd.read_html(response.text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seel all the tables available and find the one you want\n",
    "\n",
    "# for index, table in enumerate(scraper):\n",
    "#     print(\"***********************************************************\")\n",
    "#     print(\"Index\", index)\n",
    "#     print(table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use rex to make \"<!-- \\tGenLink2stk('AS2891','中信金'); //-->\" to \"2891中信金\"\n",
    "def get_stock_id_name(text):\n",
    "    m = re.search(r\"GenLink2stk\\('AS(\\d+)','(.+?)'\\);\", text)\n",
    "    if m:\n",
    "        return m.group(1) + m.group(2)\n",
    "    else:\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use rex to add \"買超\" to the beginning of the column name\n",
    "def add_buy(text):\n",
    "    return \"買超-\" + text\n",
    "\n",
    "# make the above code a function with a parameter of dataframe\n",
    "def process_table(df):\n",
    "    # apply to the first column of table\n",
    "    stock_id_name = df.iloc[:, 0].apply(get_stock_id_name)\n",
    "    # put it back to the table\n",
    "    df.iloc[:, 0] = stock_id_name\n",
    "    # remove the first row\n",
    "    df = df.iloc[1:] \n",
    "    # use the first row as header\n",
    "    df.columns = df.iloc[0]\n",
    "    # remove the first row again\n",
    "    df = df.iloc[1:]\n",
    "    # add \"買超-/賣超-\" to the beginning of the column name\n",
    "    df.columns = df.columns.map(add_buy)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_buy = process_table(scraper[3])\n",
    "\n",
    "# df_sell = process_table(scraper[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# edit the column of \"買超-券商名稱\". make it separate digits and letters. both of them are strings\n",
    "def separate_digits_letters(text):\n",
    "    m = re.search(r\"(\\d+)([a-zA-Z]+)\", text)\n",
    "    if m:\n",
    "        return m.group(1), m.group(2)\n",
    "    else:\n",
    "        return text, \"\"\n",
    "    \n",
    "df_buy[\"買超-券商代號\"], df_buy[\"買超-券商名稱\"] = zip(*df_buy[\"買超-券商名稱\"].map(separate_digits_letters))\n",
    "\n",
    "# make \"買超-券商代號\" the first column\n",
    "df_buy = df_buy[[\"買超-券商代號\", \"買超-券商名稱\"] + [col for col in df_buy.columns if col not in [\"買超-券商代號\", \"買超-券商名稱\"]]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get today and yeaterday's all stocks data from [Goodinfo](https://goodinfo.tw/tw2/StockList.asp?MARKET_CAT=%E7%86%B1%E9%96%80%E6%8E%92%E8%A1%8C&INDUSTRY_CAT=%E6%88%90%E4%BA%A4%E5%83%B9+%28%E9%AB%98%E2%86%92%E4%BD%8E%29%40%40%E6%88%90%E4%BA%A4%E5%83%B9%40%40%E7%94%B1%E9%AB%98%E2%86%92%E4%BD%8E)\n",
    "\n",
    "- POST method\n",
    "\n",
    "- Query parameters\n",
    "    - MARKET_CAT\n",
    "    - INDUSTRY_CAT\n",
    "    - RPT_TIME\n",
    "    - RANK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get yesterday's date\n",
    "yesterday = datetime.date.today() - datetime.timedelta(days=1)\n",
    "yesterday = yesterday.strftime(\"%Y/%m/%d\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all stocks data (today)\n",
    "# Initialize the final dataframe\n",
    "all_stocks = pd.DataFrame()\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36',\n",
    "}\n",
    "\n",
    "for rank in range(0, 8): # (0, 8)\n",
    "    url = f\"https://goodinfo.tw/tw2/StockList.asp?MARKET_CAT=%E7%86%B1%E9%96%80%E6%8E%92%E8%A1%8C&INDUSTRY_CAT=%E6%88%90%E4%BA%A4%E5%83%B9+%28%E9%AB%98%E2%86%92%E4%BD%8E%29%40%40%E6%88%90%E4%BA%A4%E5%83%B9%40%40%E7%94%B1%E9%AB%98%E2%86%92%E4%BD%8E&RANK={rank}\"\n",
    "    \n",
    "    response = requests.post(url, headers=headers)\n",
    "    response.encoding = 'uft8'  # Specify the correct encoding here\n",
    "    \n",
    "    scraper = pd.read_html(response.text)\n",
    "    \n",
    "    stocks = scraper[61]\n",
    "    \n",
    "    all_stocks = pd.concat([all_stocks, stocks], axis=0)\n",
    "\n",
    "    print(f\"Fetched {rank} page for all_stocks\")\n",
    "    \n",
    "    time.sleep(10)\n",
    "    \n",
    "    \n",
    "# delete the rows where \"名稱\" == \"名稱\"\n",
    "all_stocks = all_stocks[all_stocks[\"名稱\"] != \"名稱\"]    \n",
    "\n",
    "# delete the columns which have all NaN\n",
    "all_stocks = all_stocks.dropna(axis=1, how=\"all\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all stocks yesterday\n",
    "\n",
    "# get yesterday's date\n",
    "yesterday = datetime.date.today() - datetime.timedelta(days=1)\n",
    "yesterday = yesterday.strftime(\"%Y/%m/%d\") # e.g. '2024/03/22'\n",
    "\n",
    "# Initialize the final dataframe\n",
    "all_stocks_yesterday = pd.DataFrame()\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36',\n",
    "}\n",
    "\n",
    "for rank in range(0, 8): # (0, 8) -> 2m16s\n",
    "    url = f\"https://goodinfo.tw/tw2/StockList.asp?MARKET_CAT=%E7%86%B1%E9%96%80%E6%8E%92%E8%A1%8C&INDUSTRY_CAT=%E6%88%90%E4%BA%A4%E5%83%B9+%28%E9%AB%98%E2%86%92%E4%BD%8E%29%40%40%E6%88%90%E4%BA%A4%E5%83%B9%40%40%E7%94%B1%E9%AB%98%E2%86%92%E4%BD%8E&RANK={rank}&RPT_TIME={yesterday}\"\n",
    "    \n",
    "    response = requests.post(url, headers=headers)\n",
    "    response.encoding = 'uft8'  # Specify the correct encoding here\n",
    "    \n",
    "    scraper = pd.read_html(response.text)\n",
    "    \n",
    "    stocks = scraper[61]\n",
    "    \n",
    "    all_stocks_yesterday = pd.concat([all_stocks, stocks], axis=0)\n",
    "\n",
    "    print(f\"Fetched {rank} page for all_stocks_yesterday\")\n",
    "    \n",
    "    time.sleep(10)\n",
    "    \n",
    "    \n",
    "# delete the rows where \"名稱\" == \"名稱\"\n",
    "all_stocks_yesterday = all_stocks[all_stocks[\"名稱\"] != \"名稱\"]    \n",
    "\n",
    "# delete the columns which have all NaN\n",
    "all_stocks_yesterday = all_stocks.dropna(axis=1, how=\"all\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only \"代號\", \"成交 張數\" and  \"成交額 (百萬)\" in all_stocks_yesterday\n",
    "all_stocks_yesterday = all_stocks_yesterday[[\"代號\", \"成交 張數\", \"成交額 (百萬)\"]]\n",
    "all_stocks_yesterday.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename \"成交 張數\" and  \"成交額 (百萬)\" to \"前一日成交 張數\" and  \"前一日成交額 (百萬)\"\n",
    "all_stocks_yesterday.columns = [\"代號\", \"前一日成交 張數\", \"前一日成交額 (百萬)\"]\n",
    "all_stocks_yesterday.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge all_stocks and all_stocks_yesterday based on \"代號\"\n",
    "all_stocks = all_stocks.merge(all_stocks_yesterday, on=\"代號\", how=\"left\")\n",
    "all_stocks.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge df_buy and all_stocks based on \"買超-券商代號\" and \"代號\"\n",
    "df_buy = df_buy.merge(all_stocks, left_on=\"買超-券商代號\", right_on=\"代號\", how=\"left\")\n",
    "df_buy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save df_buy to excel file in a sheet named \"買超\"\n",
    "df_buy.to_excel(\"券商進出排行_draft.xlsx\", sheet_name=\"買超\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pandasScraper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
